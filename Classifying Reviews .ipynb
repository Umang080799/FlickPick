{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer ##For removing punctuation \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.svm import SVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Classifical Convention of stemming, converting to lowercase , removing stopwords and then using the SVM model to train the movie ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Importing the movie reviews \n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Building the list of documents\n",
    "##There are 2000 movie reviews with 1000 positive and 1000 negative movie reviews sorted out together.\n",
    "document = []\n",
    "category = []\n",
    "print(movie_reviews.categories())\n",
    "\n",
    "for category in movie_reviews.categories():\n",
    "    if category != 'neg':\n",
    "        print(len(movie_reviews.fileids(category)))\n",
    "\n",
    "\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        if category == 'neg' or category != 'neg':\n",
    "            document.append(movie_reviews.words(fileid))\n",
    "\n",
    "print('First Review: {}'.format(' '.join(document[10])))\n",
    "print('Total Length : {}'.format(len(document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Words used : [(u'film', 11201), (u'movi', 6980), (u'one', 6030), (u'like', 4137), (u'charact', 3881), (u'make', 3243), (u'get', 3220), (u'time', 3047), (u'scene', 2671), (u'even', 2611), (u'good', 2475), (u'play', 2382), (u'stori', 2346), (u'see', 2224), (u'would', 2110), (u'much', 2051), (u'go', 2015), (u'well', 1968), (u'also', 1967), (u'two', 1912)]\n"
     ]
    }
   ],
   "source": [
    "##Creating a tuple with word tokenized review and the category for each of the 2000 documents \n",
    "documents = [(list(movie_reviews.words(fileid)),category)\n",
    "            for category  in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "ps = PorterStemmer() ##For stemming \n",
    "stopwords = set(stopwords.words('english')) ##For stopwords\n",
    "clean_words = []\n",
    "    \n",
    "    \n",
    "##Shuffling the documents\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    if w not in stopwords:\n",
    "        w = filter(unicode.isalnum, w)\n",
    "        if w != '':\n",
    "            w = ps.stem(w) ##Stemming the words \n",
    "            all_words.append(w.lower())\n",
    "    \n",
    "\n",
    "all_words = nltk.FreqDist(all_words)  ##Arranges in the order of most common words \n",
    "\n",
    "\n",
    "print('Most Common Words used : {}'.format(all_words.most_common(20)))\n",
    "# print len(documents[0][0])\n",
    "# print len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot\n",
      "music\n",
      "want\n",
      "arrow\n",
      "away\n",
      "bottom\n",
      "down\n",
      "concept\n",
      "exact\n",
      "fuck\n",
      "still\n",
      "stir\n",
      "not\n",
      "one\n",
      "get\n",
      "3\n",
      "s\n",
      "world\n",
      "start\n",
      "with\n",
      "7\n",
      "horror\n",
      "more\n",
      "american\n",
      "also\n",
      "dead\n",
      "deal\n",
      "into\n",
      "video\n",
      "insight\n",
      "off\n",
      "lost\n",
      "problem\n",
      "blair\n"
     ]
    }
   ],
   "source": [
    "##Out of all the words, I'm using the most common 4000 words as features\n",
    "\n",
    "word_features = (all_words.keys())[:4000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document) ##Contains the word tokenizers present in document \n",
    "    features = {} ##empty dictionary\n",
    "    \n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)  ##th\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "features = find_features(movie_reviews.words('neg/cv000_29416.txt'))\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print key\n",
    "\n",
    "##words like proble, horror, confusing etc could be used as a feature to give a negative feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "##making the feature list for all the documents\n",
    "\n",
    "feature_sets = [(find_features(rev),category) for rev,category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "##Splitting the data into training and Testing\n",
    "\n",
    "seed = 1\n",
    "\n",
    "train,test = model_selection.train_test_split(feature_sets, test_size = 0.25, random_state = seed)\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SklearnClassifier(SVC(kernel= 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 0.692\n"
     ]
    }
   ],
   "source": [
    "accuracy  = nltk.classify.accuracy(model, test)\n",
    "print('SVC Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out tf-idf and Count Vectorizer technique on real movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_vector = CountVectorizer(min_df = 2, tokenizer = nltk.word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviedata = r'/home/umang/Desktop/NLP/corpora/movie_reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all files as training data. \n",
    "movie_data = load_files(moviedata, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(movie_data.data))\n",
    "movie_data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##The 1001th review in the movie_review corpus is a positive review\n",
    "movie_data.target[1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 2 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(2000, 25286)\n"
     ]
    }
   ],
   "source": [
    "##The tf vector \n",
    "movie_counts = movie_vector.fit_transform(movie_data.data)\n",
    "print(movie_counts.todense())\n",
    "print (movie_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.03844927, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Making the tf-idf vector\n",
    "movie_tfidf = TfidfTransformer().fit_transform(movie_counts)\n",
    "movie_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 25286)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##There are 25286 unique indexed term in the vocabulary\n",
    "movie_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1 represents pos whereas 0 represents negative \n",
    "movie_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model using the Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test = train_test_split(\n",
    "    movie_tfidf, movie_data.target, test_size = 0.20, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fitting the model on the training data\n",
    "model_nb = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy through Naive Bayes Classifier is : 0.82\n"
     ]
    }
   ],
   "source": [
    "##Predicitng the model on the testing data and then determining the accuracy\n",
    "predictions = model_nb.predict(X_test)\n",
    "print(\"Accuracy through Naive Bayes Classifier is : {}\".format(sklearn.metrics.accuracy_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix:[[175  31]\n",
      " [ 41 153]]\n"
     ]
    }
   ],
   "source": [
    "##Printing out the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print (\"Matrix:{}\".format(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model using the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test = train_test_split(\n",
    "    movie_tfidf, movie_data.target, test_size = 0.20, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Fitting the model on the training data\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy through Logistic Regression is : 0.7825\n"
     ]
    }
   ],
   "source": [
    "##Predicitng the model on the testing data and then determining the accuracy\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Accuracy through Logistic Regression is : {}\".format(sklearn.metrics.accuracy_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix:[[152  54]\n",
      " [ 33 161]]\n"
     ]
    }
   ],
   "source": [
    "##Printing out the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print (\"Matrix:{}\".format(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We conclude that the Naive Bayes model with tf-idf features gives us the best results.\n",
    "## We now predict the classification result based on a set of random reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    " # very short and fake movie reviews\n",
    "reviews = ['Awesome Horror movie','This movie was aweful', 'Absolute boring ride', \n",
    "            'awesome','YOLO', 'Steven Seagal shined through.', ':(',\n",
    "              'This was certainly a movie', 'Two thumbs awesome up', 'I fell asleep halfway through', \n",
    "              \"We can't wait for the sequel!!\", '!', '?', 'I cannot recommend this highly enough', \n",
    "              'instant classic.', 'Steven Seagal was amazing. His performance was Oscar-worthy.','Just fucking awesome.']\n",
    "reviews_new_counts = movie_vector.transform(reviews)\n",
    "reviews_new_tfidf = TfidfTransformer().fit_transform(reviews_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25286"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 25286)\n"
     ]
    }
   ],
   "source": [
    "print (reviews_new_tfidf.todense().shape)\n",
    "prediction = model_nb.predict(reviews_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Awesome Horror movie' => pos\n",
      "'This movie was aweful' => neg\n",
      "'Absolute boring ride' => neg\n",
      "'awesome' => pos\n",
      "'YOLO' => pos\n",
      "'Steven Seagal shined through.' => neg\n",
      "':(' => neg\n",
      "'This was certainly a movie' => neg\n",
      "'Two thumbs awesome up' => pos\n",
      "'I fell asleep halfway through' => neg\n",
      "\"We can't wait for the sequel!!\" => neg\n",
      "'!' => neg\n",
      "'?' => neg\n",
      "'I cannot recommend this highly enough' => neg\n",
      "'instant classic.' => pos\n",
      "'Steven Seagal was amazing. His performance was Oscar-worthy.' => pos\n",
      "'Just fucking awesome.' => neg\n"
     ]
    }
   ],
   "source": [
    "for review, segregation in zip(reviews_new, prediction):\n",
    "    print('%r => %s' % (review, movie_data.target_names[segregation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
